# >:]

# name = rss.ticker.fetch.multiple

my $rss_url_ref = shift;
die "expected hash reference containing RSS urls"
    if not defined $rss_url_ref
    or !length($rss_url_ref)
    or ref($rss_url_ref) ne 'HASH';
my $output_file = <rss.ticker.txt_data_path>;
( my $data_dir = <rss.data.tmp_path> . '/rssdata' ) =~ s,(/|\.\.)/,/,g;
my $join_type = <rss.ticker.join_type>;
$join_type //= 'mixed';
my $seperator = <rss.ticker.item_seperator>;
<rss.ticker.cache_timeout> //= 3;    # 3 hours default timeout for cached feeds

my @urls;
map { push( @urls, $rss_url_ref->{$_} ) } keys( %{$rss_url_ref} );
if ( !@urls ) {
    <[rss.ticker.warn_urls_undefined]>;
    return;
}
<rss.ticker.warned_undefined.urls> = 0;
<[base.log]>->( 2, "checking RSS feeds for new data..." );

my %failed;
my $got_updates = 0;

foreach my $rss_index ( keys( %{$rss_url_ref} ) ) {
    my $rss_url  = $rss_url_ref->{$rss_index};
    my $response = <rss.browser>->get($rss_url);
    if ( $response->code == 200 ) {

        <[base.log]>->( 2, ": got fresh RSS content..." );

        my $response_content
            = $response->decoded_content( 'charset' => 'none' );

        my $err_str;
        my @txt_data;
        my $feed = XML::Feed->parse( \$response_content )
            or $err_str = XML::Feed->errstr;

        if ( defined $err_str ) {
            <[base.log]>->(
                0, "[!] feed parser error on URL $rss_index [ $err_str ]"
            );
            @txt_data = ("[!] failed to parse feed #$rss_index [!]");
        } else {
            eval {
                foreach my $entry ( $feed->entries ) {
                    push( @txt_data, $entry->title );
                }
            };
        }
        my $url_hash = <[digest.sha1]>->($rss_url);
        if ( !-d $data_dir ) {
            mkdir($data_dir)
                or die "can't create rss data dir '$data_dir'! [$!]";
            chmod( 0700, $data_dir );
        }
        my $url_data_file .= "$data_dir/$url_hash.txt";
        open( my $txt_fh, '>' . $url_data_file )
            or die "can't write to feed data file [$!]";
        {
            print {$txt_fh} join( "\n", @txt_data ) . "\n";
        }
        close($txt_fh);

        $got_updates++;
        <[base.log]>->( 2, ": : wrote txt data to '$url_data_file'" );
        <rss.browser>->commit;

    } elsif ( $response->code == 304 ) {
        <[base.log]>->( 2, ": no new content since last request" );
    } else {
        <[base.log]>->(
            0,
            "failed to access RSS url '$rss_url' ("
                . $response->status_line . ")"
        );
        $failed{$rss_url} = 1;
    }
}
return 0 if !$got_updates and !keys(%failed);
my %txt_data;
my $feed_data_count = 0;
foreach my $index ( keys( %{$rss_url_ref} ) ) {
    my $rss_url  = $rss_url_ref->{$index};
    my $url_hash = <[digest.sha1]>->($rss_url);
    my $url_data_file .= "$data_dir/$url_hash.txt";

    if ( !-f $url_data_file ) {
        <[base.log]>->(
            0, "no cached version of feed #$index present.. skipping.."
        );
        next;
    }

    if ( $failed{$rss_url} ) {
        my ($dev,  $ino,   $mode,  $nlink, $uid,     $gid, $rdev,
            $size, $atime, $mtime, $ctime, $blksize, $blocks
        ) = stat($url_data_file);
        my $file_age = sprintf( "%.2f", ( time() - $ctime ) / 3600 );
        if ( $file_age >= <rss.ticker.cache_timeout> ) {
            <[base.log]>->(
                0,
                "cached version of feed #$index has timed out ($file_age hours)"
            );
            unlink($url_data_file);
            next;
        } else {
            <[base.log]>->(
                1, "using cached version of feed #$index ($file_age hours old)"
            );
        }
    }

    open( my $r_fh, '<' . $url_data_file )
        or die "can't open feed data file '$url_data_file'! [$!]";
    @{ $txt_data{$index} } = <$r_fh>;
    close($r_fh);
    $feed_data_count++;
}

if ( !$feed_data_count ) {
    <[base.log]>->(
        0, "<!> WARNING: no feed data available, ticker file empty!"
    );
    <rss.ticker.fail_retry_interval> //= <rss.ticker.fail_retry_min_interval>;
    <rss.ticker.update_timer>->cancel;
    <rss.ticker.update_timer> = <[event.add_timer]>->(
        {   'after'  => 4.2,    # failure retry (quickfix)
            'repeat' => 1,
            'interval' => <rss.ticker.update_interval> || 120,
            'handler'  => 'rss.ticker.fetch_content',
            'data'     => <rss.ticker.update_commands>
        }
    );
    <rss.ticker.fail_retry_interval> *= 1.1;
    <rss.ticker.fail_retry_interval> = <rss.ticker.fail_retry_max_interval>
        if <rss.ticker.fail_retry_interval>
        > <rss.ticker.fail_retry_max_interval>;
    return -2;
}

open( my $txt_fh, '>', $output_file . '.NEW' )
    or die "can't write to ticker data file '$output_file.NEW' [$!]";
my @output_data;
if ( $join_type eq 'mixed' ) {
    while ( keys(%txt_data) ) {
        foreach my $txt_index ( sort keys(%txt_data) ) {
            push( @output_data, shift( @{ $txt_data{$txt_index} } ) );
            delete $txt_data{$txt_index} if !@{ $txt_data{$txt_index} };
        }
    }
    push( @output_data, '' ) if <rss.ticker.trailing_seperator>;
    print {$txt_fh} join( " $seperator ", @output_data );
} elsif ( $join_type eq 'seperate' ) {
    foreach my $txt_index ( sort keys(%txt_data) ) {
        push( @output_data,
            join( " $seperator ", @{ $txt_data{$txt_index} } ) );
        push( @output_data, "^\n" );
    }
    pop(@output_data);
    print {$txt_fh} @output_data;
} else {
    <[base.log]>->(
        0, "unknown join type '$join_type' [expected 'mixed' or 'seperate']"
    );
    close($txt_fh);
    unlink($output_file);
    return -1;
}
close($txt_fh);
move( $output_file . '.NEW', $output_file )
    or die "can't move '$output_file.NEW' to '$output_file' [$!]";
<[base.log]>->( 2, ": : wrote txt data to '$output_file'" );
return 1;
